{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "format:\n",
        "  html:\n",
        "    self-contained: true\n",
        "---"
      ],
      "id": "64e2d2bc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup\n",
        "\n",
        "Cluster is setup as usual using Google Cloud's DataProc.\n",
        "\n",
        "One launched the R and python environment are updated via conda:\n",
        "\n",
        "```shell\n",
        "conda upgrade --all\n",
        "```\n",
        "\n",
        "`SparkR` should be already installed and we can install RStudio's `sparklyr` with\n"
      ],
      "id": "947d8b65"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "install.packages(\"sparklyr\")"
      ],
      "id": "d0fb4001",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also for good measure I've installed quarto,\n",
        "\n",
        "```\n",
        "wget https://github.com/quarto-dev/quarto-cli/releases/download/v1.1.251/quarto-1.1.251-linux-amd64.deb\n",
        "dpkg -i quarto-1.1.251-linux-amd64.deb\n",
        "```\n",
        "\n",
        "## Data\n",
        "\n",
        "NYC taxi data from https://github.com/toddwschneider/nyc-taxi-data\n",
        "\n",
        "```shell\n",
        "mkdir nyc_taxi\n",
        "cd nyc_taxi\n",
        "\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet\n",
        "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet\n",
        "```\n",
        "\n",
        "Move to hdfs,\n",
        "\n",
        "```shell\n",
        "hdfs dfs -mkdir nyc_taxi\n",
        "hdfs dfs -put *.parquet nyc_taxi/\n",
        "```\n",
        "\n",
        "```shell\n",
        "hdfs dfs -ls nyc_taxi/\n",
        "## Found 18 items\n",
        "## -rw-r--r--   2 root hadoop   21686067 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-01.parquet\n",
        "## -rw-r--r--   2 root hadoop   21777258 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-02.parquet\n",
        "## -rw-r--r--   2 root hadoop   30007852 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-03.parquet\n",
        "## -rw-r--r--   2 root hadoop   34018560 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-04.parquet\n",
        "## -rw-r--r--   2 root hadoop   38743682 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-05.parquet\n",
        "## -rw-r--r--   2 root hadoop   44071592 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-06.parquet\n",
        "## -rw-r--r--   2 root hadoop   43697690 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-07.parquet\n",
        "## -rw-r--r--   2 root hadoop   43425907 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-08.parquet\n",
        "## -rw-r--r--   2 root hadoop   46125883 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-09.parquet\n",
        "## -rw-r--r--   2 root hadoop   53286464 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-10.parquet\n",
        "## -rw-r--r--   2 root hadoop   53100722 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-11.parquet\n",
        "## -rw-r--r--   2 root hadoop   49639052 2022-09-20 15:43 nyc_taxi/yellow_tripdata_2021-12.parquet\n",
        "## -rw-r--r--   2 root hadoop   38139949 2022-09-20 15:41 nyc_taxi/yellow_tripdata_2022-01.parquet\n",
        "## -rw-r--r--   2 root hadoop   45616512 2022-09-20 15:41 nyc_taxi/yellow_tripdata_2022-02.parquet\n",
        "## -rw-r--r--   2 root hadoop   55682369 2022-09-20 15:41 nyc_taxi/yellow_tripdata_2022-03.parquet\n",
        "## -rw-r--r--   2 root hadoop   55222692 2022-09-20 15:41 nyc_taxi/yellow_tripdata_2022-04.parquet\n",
        "## -rw-r--r--   2 root hadoop   55558821 2022-09-20 15:41 nyc_taxi/yellow_tripdata_2022-05.parquet\n",
        "## -rw-r--r--   2 root hadoop   55365184 2022-09-20 15:41 nyc_taxi/yellow_tripdata_2022-06.parquet\n",
        "```\n",
        "\n",
        "Since relative paths were used the data lives in the current user's home directory (happens to be root here). If you want to track it down you can do something like the following,\n",
        "\n",
        "```shell\n",
        "hdfs dfs -find / -name \"nyc_taxi\"\n",
        "## /user/root/nyc_taxi\n",
        "```\n",
        "\n",
        "## SparkR\n"
      ],
      "id": "567d1c95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "library(SparkR)"
      ],
      "id": "09122372",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we can use Spark we need to create a Spark Session which will connect to the existing cluster, here we use `master = \"yarn\"` since we are using yarn for job management.\n"
      ],
      "id": "1d259058"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SparkR::sparkR.session(master = \"yarn\")"
      ],
      "id": "a8a8d0bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data can be read locally or from hdfs - the package provides various `read.*()` functions, since our data is parquet we will use `read.parquet()`.\n"
      ],
      "id": "18bad321"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "(d = SparkR::read.parquet(\"hdfs:///user/root/nyc_taxi/*.parquet\"))"
      ],
      "id": "f1b44665",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "head(d)"
      ],
      "id": "2dc9fe5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dim(d)"
      ],
      "id": "b4abd7e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SparkR::schema(d)"
      ],
      "id": "ca29d4a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic data processing\n",
        "\n",
        "SparkR's basic interface is just a poor implementation of dplyr and the basic words - most everything works how you would expect with the primary exception being the NSE is not the same.\n"
      ],
      "id": "8d43b5a7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "library(magrittr)"
      ],
      "id": "d834216f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    summarize(\n",
        "        avg_n = mean(d$passenger_count),\n",
        "        avg_fare = mean(d$fare_amount),\n",
        "        avg_tip = mean(d$tip_amount / d$fare_amount)\n",
        "    )"
      ],
      "id": "cbb83d35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    summarize(\n",
        "        avg_n = mean(d$passenger_count),\n",
        "        avg_fare = mean(d$fare_amount),\n",
        "        avg_tip = mean(d$tip_amount / d$fare_amount)\n",
        "    ) %>%\n",
        "    collect()"
      ],
      "id": "3096090e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    group_by(\"VendorID\") %>%\n",
        "    summarize(\n",
        "        avg_n = mean(d$passenger_count),\n",
        "        avg_fare = mean(d$fare_amount),\n",
        "        avg_tip = mean(d$tip_amount / d$fare_amount)\n",
        "    ) %>%\n",
        "    collect()"
      ],
      "id": "81bc5888",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More advanced data processing\n",
        "\n",
        "Spark is not R so of the common operations or functions that we might want / expect are not going to be there - in which case we need to find the equivalent spark function for our purpose.\n",
        "\n",
        "A good place to look is at the function reference found [here](https://spark.apache.org/docs/latest/api/R/reference/index.html)\n"
      ],
      "id": "1c6301a0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    mutate(duration = d$tpep_dropoff_datetime - d$tpep_pickup_datetime) %>%\n",
        "    select(\"duration\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\") %>%\n",
        "    head()    "
      ],
      "id": "e974ab7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    mutate(duration = d$tpep_dropoff_datetime - d$tpep_pickup_datetime) %>%\n",
        "    select(\"duration\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\") %>%\n",
        "    head() %>% \n",
        "    {.[[\"duration\"]][[1]]}"
      ],
      "id": "cea529fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    mutate(duration = datediff(d$tpep_dropoff_datetime, d$tpep_pickup_datetime)) %>%\n",
        "    select(\"duration\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\") %>%\n",
        "    head()"
      ],
      "id": "763c7439",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    mutate(duration = as.integer(d$tpep_dropoff_datetime) - as.integer(d$tpep_pickup_datetime)) %>%\n",
        "    select(\"duration\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\") %>%\n",
        "    head()"
      ],
      "id": "ecb21891",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    mutate(duration = (cast(d$tpep_dropoff_datetime, \"double\") - cast(d$tpep_pickup_datetime, \"double\")) / 60) %>%\n",
        "    select(\"duration\", \"tpep_dropoff_datetime\", \"tpep_pickup_datetime\") %>%\n",
        "    head()"
      ],
      "id": "4072662f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## sparklyr\n",
        "\n",
        "This is RStudio's effort in this space - it leverages the existing work on dplyr + dbplyr to interface with spark / hdfs. As such it is performing basically the same trick in so far as your dplyr code is being translated into SQL and then Spark is being interacted with via its SQL interface.\n",
        "\n",
        "Like with SparkR (and other R database connections) we need to create a session / connection first,\n"
      ],
      "id": "9a3e2a59"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "library(sparklyr)\n",
        "library(dplyr)"
      ],
      "id": "6089f692",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sc = sparklyr::spark_connect(master = \"yarn\")"
      ],
      "id": "df2e02b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d = sparklyr::spark_read_parquet(sc, \"taxi\", \"hdfs:///user/root/nyc_taxi/*.parquet\")"
      ],
      "id": "0c20156c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d"
      ],
      "id": "39e5dd53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dim(d)"
      ],
      "id": "3d8ab2dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>% \n",
        "    summarize(n())"
      ],
      "id": "fe9bfc8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    group_by(VendorID) %>%\n",
        "    summarize(\n",
        "        avg_passengers = mean(passenger_count),\n",
        "        avg_tip = mean(tip_amount/fare_amount)\n",
        "    )"
      ],
      "id": "65c7f8c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    mutate(\n",
        "        duration = tpep_dropoff_datetime - tpep_pickup_datetime\n",
        "    ) %>%\n",
        "    select(duration, tpep_dropoff_datetime, tpep_pickup_datetime)"
      ],
      "id": "321a4747",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    mutate(\n",
        "        duration = as.double(tpep_dropoff_datetime - tpep_pickup_datetime)\n",
        "    ) %>%\n",
        "    select(duration, tpep_dropoff_datetime, tpep_pickup_datetime)"
      ],
      "id": "2db7a83a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d %>%\n",
        "    mutate(\n",
        "        duration = (as.double(tpep_dropoff_datetime) - as.double(tpep_pickup_datetime))/60\n",
        "    ) %>%\n",
        "    select(duration, tpep_dropoff_datetime, tpep_pickup_datetime)"
      ],
      "id": "5897d465",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}