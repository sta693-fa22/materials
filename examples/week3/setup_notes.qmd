---
title: "Setup Notes - Week 3"
format: 
  html:
    self-contained: true
---

## Google Dataproc

Standard (defaut) configuration - couple things to consider:

* Pick region and zone

* Pick config for Master node

* Pick # and config or worker nodes

  * Persistent storage size determines size of hdfs pool 

* Good idea to include jupyter as an optional component

* Jupyter accessible via `Cluster details` > `Web interfaces`


## Copy data to the master node

In Jupyter's terminal,

```
mkdir /data
cd /data

wget https://raw.githubusercontent.com/sta693-fa22/materials/main/examples/week3/hamlet.txt

wget https://raw.githubusercontent.com/sta693-fa22/materials/main/examples/week3/shakespeare.zip
```


Also we can copy the mrjob scripts, more on this in a bit

```
wget https://raw.githubusercontent.com/sta693-fa22/materials/main/examples/week3/shakespeare.zip -O shakespeare.zip

wget https://raw.githubusercontent.com/sta693-fa22/materials/main/examples/week3/shakespeare.zip -O shakespeare.zip

```

### Jupyter + File System 

Will start in `/GCS/` which may be unclear - this is the staging bucket for your cluster, can be seen with:

```
gsutil ls
# gs://dataproc-staging-us-east4-1034449174331-0ejz78jy/
# gs://dataproc-temp-us-east4-1034449174331-odqdujvl/
```

actual path of `/GCS` is `gs://dataproc-staging-us-east4-1034449174331-0ejz78jy/notebooks/jupyter/`.

Usual file system commands will work with `gsutil` preface - e.g. `gsutil ls`, `gsutil cp`, `gsutil rm`, etc.


### mrjob

The required python package is not installed already - can be added via 

```
pip3 install mrjob
```

note `pip3` is necessary here otherwise it will be installed for `python2` and not `python3`.


### hdfs

Dataproc takes care of installation and configuration, should be accessible via `hdfs`,

```
hdfs dfs -ls hdfs:///

hdfs dfs -mkdir hdfs:///data/

hdfs dfs -ls hdfs://
hdfs dfs -ls hdfs:///data/

hdfs dfs -put ./hamlet.txt hdfs:///data/
hdfs dfs -ls hdfs:///data/

hdfs dfs -get hdfs:///data/hamlet.txt ./hamlet2.txt
```

### Running MapReduce jobs with mrjobs

mrjob can run the job using hadoop - we just need to specify the runner with `-r` and provide the path to the inputs on the hdfs.

```
python3 word_count.py -r hadoop hdfs:///data/hamlet.txt
```